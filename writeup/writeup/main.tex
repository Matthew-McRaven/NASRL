%%%%%%%% ICML 2020 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2020}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2020}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2020}

\begin{document}

\twocolumn[
\icmltitle{Deep Meta Reinforcement Learning for Neural Architecture Search in Image Classification}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2020
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Connor Lu}{equal,gt}
\icmlauthor{Matthew McRaven}{equal,gt}
\end{icmlauthorlist}

\icmlaffiliation{gt}{Department of Computer Science, Georgetown University, District of Columbia, USA}

%TODO: Need Connor's email
\icmlcorrespondingauthor{Connor Lu}{cyl28@georgetown.edu}
\icmlcorrespondingauthor{Matthew McRaven}{mkm302@georgetown.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{meta reinforcement learning, neural architecture search, image classification}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We are interested in deep meta-reinforcement learning, so we searched for a topic in this domain.
Additionally, we have existing in the image classification domain.
In recent years, one application of meta-learning to image classification has been meta-learning which architectures efficiently solve a given task.
This is known as neural architecture search (NAS), and our project proposal explores this topic.
\end{abstract}


\vspace{-2em}
\section{Background}

One interesting subfield of deep meta reinforcement learning is NAS.
The goal of NAS is to design a RL agent whose output is a neural net architecture well suited for a particular problem.
 One particular NAS implementation, NASNet, achieved state-of-the-art performance on CIFAR-10\cite{zoph2016neural}.
 The network architectures generated by NASNet contained fewer parameters than human-designed network architectures, which enables proliferation of the model to lower-power devices.
 
More recent advancements in the field increase the granularity of the agent’s control over the generated network.
Ramachandran et al. explored creating new activation functions for neural nets, and their results, when applied to existing architectures, lead to improved accuracy with no changes to network architecture\cite{ramach2017searching}.
Zoph et al. later studied the use of NAS to create neural architectures that exhibit positive transfer between datasets\cite{Zoph_2018}.
While Gaier \& Ha used neuroevolution rather than NAS, they found that certain neural architectures, when randomly initialized and

\section{Project Proposal}
All of the above literature suggest something interesting: it is possible to find neural architectures that are uniquely equipped to solve a particular problem.
We propose the task of implementing NAS to generate image classification networks; we plan to use MNist, CIFAR-10, and ImageNet datasets to evaluate the generated networks.
These image datasets are readily available, have varying complexities, and provide an easy way for a human to judge performance.
This technique has nearly exclusively been applied to image-related tasks--it is unclear how to extend it to text processing.

We want to experiment with NAS because it is a promising, state-of-the-art technique that is motivated by the attempt to avoid feature engineering—indeed, this technique aims to avoid configuring the neural net architecture altogether.
This is a meta-learning task: we plan to implement a RNN that generates NNs, which will then be trained for some number of epochs; the goal is to train the RNN to output networks with high performance.
This is also a meta-RL task, as REINFORCE will be used to reward/penalize the RNN based on the performance of its generated networks.

\section{Deliverables}
During this project, we plan on delivering three separate agents trained on the CIFAR-10 dataset:
\vspace{-1em}
\begin{itemize}
	\item A meta-learner that generates exclusively fully-connected layers to perform classification. 
	\vspace{-1em}
	\item A meta-learner that generates convolutional and pooling layers, followed by fully-connected layers to perform classification.
	\vspace{-1em}
	\item A meta-learner that generates convolutional, pooling, convolutional-transpose, and fully connected layers to perform auto-encoding.
\end{itemize}
\vspace{-1em}

\bibliography{../sources}
\bibliographystyle{icml2020}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2020. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
